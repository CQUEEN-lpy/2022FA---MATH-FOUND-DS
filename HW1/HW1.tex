
\documentclass[11pt,letterpaper]{article}
\textwidth 6.5in
\textheight 9.in
\oddsidemargin 0in
\headheight 0in
\usepackage{graphicx}
\usepackage{fancybox}
\usepackage[utf8]{inputenc}
\usepackage{epsfig,graphicx}
\usepackage{multicol,pst-plot}
\usepackage{ragged2e}
\usepackage{pstricks}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{eucal}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\pagestyle{empty}

\usepackage{algpseudocode}
\usepackage{algorithm}


\DeclareMathOperator{\tr}{Tr}
\newcommand*{\op}[1]{\check{\mathbf#1}}
\newcommand{\bra}[1]{\langle #1 |}
\newcommand{\ket}[1]{| #1 \rangle}
\newcommand{\braket}[2]{\langle #1 | #2 \rangle}
\newcommand{\mean}[1]{\langle #1 \rangle}
\newcommand{\opvec}[1]{\check{\vec #1}}
\renewcommand{\sp}[1]{$${\begin{split}#1\end{split}}$$}

\usepackage{parskip}
\usepackage{listings}
\usepackage{color}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
	backgroundcolor=\color{backcolour},   
	commentstyle=\color{codegreen},
	keywordstyle=\color{magenta},
	numberstyle=\tiny\color{codegray},
	stringstyle=\color{codepurple},
	basicstyle=\footnotesize,
	breakatwhitespace=false,         
	breaklines=true,                 
	captionpos=b,                    
	keepspaces=true,                 
	numbers=left,                    
	numbersep=5pt,                  
	showspaces=false,                
	showstringspaces=false,
	showtabs=false,                  
	tabsize=2
}

\lstset{style=mystyle}

\begin{document}
\pagestyle{plain}

\begin{flushleft}
Student Name: Xinxi Zhang\\
NetID: XZ657  \\
RUID: 219004759
\end{flushleft}

\begin{flushright}\vspace{-15mm}
\includegraphics[height=2cm]{Rutgers_Logo.png}
\end{flushright}
 
\begin{center}\vspace{-1cm}
\textbf{\large MATH FOUND DS (16:198:501)}\\
Homework 1
\end{center}

 
\rule{\linewidth}{0.1mm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bigskip
\bigskip

\begin{enumerate}

\item \justifying{
    Let $\underline\omega$ be a column vector of unknowns, $\underline\omega = (\underline\omega_1,\underline\omega_2,\dots,\underline\omega_d)$. 
    \textbf{For a given vector $\underline c$, show that the gradient of $\underline{\omega}^T\underline c$ is given by $\underline c$. For a given symmetric matrix $A$, show that the gradient of $\underline{\omega}^TA\underline\omega$ is given by $2A\underline\omega$}
}

\begin{flushleft}
Answer:
    \begin{enumerate}
        \item{
            To prove the gradient of $\underline{\omega}^T\underline c$ is given by $\underline c$: \\
            we have:\begin{align*}
                \underline\omega^T\underline c = \sum^d_{i=1} \omega_ic_i, \quad\frac{\mathrm d\omega_ic_i}{\mathrm d \omega_i}=c_i
            \end{align*}
            \begin{align*}
                \Rightarrow \quad\frac{\partial(\underline\omega^T\underline c)}{\partial\underline\omega} = c
                \\
            \Rightarrow \quad \text{$\underline{\omega}^T\underline c$ is given by $\underline c$}
            \end{align*}}

        \item{
            To prove the gradient of $\underline{\omega}^TA\underline\omega$ is given by $2A\underline\omega$: \\
            we have:
            
            \begin{align*}
                A = \begin{bmatrix}
                         | & | & | & | \\
                         \underline a_1 & \underline a_2 & \dots & \underline a_d\\
                         | & | & | & |
                    \end{bmatrix},
                \quad A^T = A, \quad a_{ij} = a_{ji}
            \end{align*}
            
            \begin{align*}
                \Rightarrow \quad A\underline\omega = A^T\underline\omega & =
                        \begin{bmatrix}
                            \underline a_1^T \underline\omega \\
                            \underline a_2^T \underline\omega \\
                            \dots \\
                            \underline a_d^T \underline\omega 
                        \end{bmatrix} 
            \end{align*}
            
            \begin{align*}
                \Rightarrow \quad \underline\omega^TA\underline\omega = \sum_{i=1}^d{
                    \underline\omega_i\underline a^T \underline\omega
                }
            \end{align*}

            \begin{align*}
                \Rightarrow \quad \frac{\partial \underline{\omega}^TA\underline\omega}{\partial \omega_i} 
                &= \sum_{j=1,j\neq i}^d{w_j\underline a_{ji}} + \sum_{j=1,j\neq i}^d{w_j\underline a_{ij}} + 2\underline a_{ij}\omega_i\\
                &= 2\sum_{j=1}^d{w_j\underline a_{ij}}\\
                &= 2\underline a_i^T\underline\omega
            \end{align*}
            \newpage

            \begin{align*}
                \Rightarrow \quad \frac{\partial \underline{\omega}^TA\underline\omega}{\partial \omega} = \begin{bmatrix}
                            2\underline a_1^T \underline\omega \\
                            2\underline a_2^T \underline\omega \\
                            \dots \\
                            2\underline a_d^T \underline\omega 
                        \end{bmatrix} = 2A\underline\omega \\
                \Rightarrow \quad \underline{\omega}^TA\underline\omega \text{ is given by } 2A\underline\omega
            \end{align*}
    }
    \end{enumerate}
\end{flushleft}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\item \begin{enumerate}
    \item For a given matrix $A$, let $G(A)$ be the matrix of derivatives so that $G_i,j = \partial/\partial A_i,j[L]$. \textbf{Show that $G = 4A(A^TA-B)$.} 
    \begin{flushleft}
        \justifying
        Firstly we can note that $B$ is a symmetric matrix because $B$ is generated by $\Tilde{A} ^T \Tilde{A}$. So we have $B_{i,j} = B_{j,i}$.
    \end{flushleft}
    
    Let:
    \begin{align*}
         A = \begin{bmatrix}
                         | & | & | & | \\
                         \underline a_1 & \underline a_2 & \dots & \underline a_d\\
                         | & | & | & |
                    \end{bmatrix}, 
        A^T = \begin{bmatrix}
                         - & \underline a_1 & -\\
                         - & \underline a_2 & -\\
                         - & \dots & -\\
                         - & \underline a_d & -\\
                    \end{bmatrix}, 
        B = \begin{bmatrix}
                         | & | & | & | \\
                         \underline b_1 & \underline b_2 & \dots & \underline b_d\\
                         | & | & | & |
                    \end{bmatrix}, 
    \end{align*}
    \begin{align*}
         G = 4A(A^TA-B) = 4(AA^TA-AB)
    \end{align*}
    \begin{align*}
         \Rightarrow \quad G_{x,y} &= 4(\sum_{i=1}^D{
            A_{x,i}*\underline a_i \cdot \underline a_y}
            - \sum_{i=1}^D{
            A_{x,i}* B_{i,y}}
            )\\
            &=4\sum_{i=1}^D{
            A_{x,i}*(\underline a_i \cdot \underline a_y-B_{i,y})}
    \end{align*}
    And we have:
    \begin{align*}
         \frac{\mathrm{d} L}{\mathrm{d} A_{x,y}} &= \frac{
            \mathrm{d} \sum_{i=1}^D \sum_{j=1}^D
            (B_{i,j} - \underline a_i \cdot \underline a_j)^2
         }{
            \mathrm{d} A_{x,y}
         }    \\
         &= \frac{
            \mathrm{d} \sum_{i=1}^D \sum_{j=1}^D
            (B_{i,j}^2 + (\underline a_i \cdot \underline a_j)^2 - 2B_{i,j}*(\underline a_i \cdot \underline a_j))
         }{
            \mathrm{d} A_{x,y}
         }\\
         &= \frac{
            \mathrm{d} \sum_{i=1}^D \sum_{j=1}^D
            ((\underline a_i \cdot \underline a_j)^2 - 2B_{i,j}*(\underline a_i \cdot \underline a_j))
         }{
            \mathrm{d} A_{x,y}
         }
    \end{align*}
    And the $A_{x,y}$ is only contained in \{$\underline a_i: i=y$\}, so:
    \begin{align*}
         \frac{\mathrm{d} L}{\mathrm{d} A_{x,y}} &= \frac{
            \mathrm{d} \sum_{j=1}^D
            ((\underline a_y \cdot \underline a_j)^2 - 2B_{x,j}*(\underline a_y \cdot \underline a_j))
            +
            \sum_{i=1}^D
            ((\underline a_i \cdot \underline a_y)^2 - 2B_{i,x}*(\underline a_i \cdot \underline a_y))
         }{
            \mathrm{d} A_{x,y}
         }\\
         &= 2\frac{
            \mathrm{d} \sum_{i=1}^D
            (\underline a_i \cdot \underline a_y)^2
         }{
            \mathrm{d} A_{x,y}
         } -
         4\frac{
            \mathrm{d} \sum_{i=1}^D B_{i,y}*(\underline a_i \cdot \underline a_y)
         }{
            \mathrm{d} A_{x,y}
         }\\
         &= 
          4\sum_{i=1}^D (A_{i,y}*(\underline a_i \cdot \underline a_y)) - 4\sum_{i=1}^D (B_{i,x}*A_{i,y}) \\
          &= 
          4\sum_{i=1}^D (A_{i,y}*(\underline a_i \cdot \underline a_y) - B_{i,y})
          \\
          &=G_{x,y}
    \end{align*}  
    \newpage

    
    \item \textbf{
    Generate a random $B$ as above, taking $D = 10$,$k = 10$, and taking $A$ as a random initial matrix, implement this process to show that $L$ decreases over time to 0 - graph your results.
    }
    \begin{algorithm}[!h]
        \caption{Gradient Descent}\label{alg:cap}
        \begin{algorithmic}
            \State $\Tilde{A} \gets $ Random Matrix(k,D)
            \State $B \gets \Tilde{A}^T \Tilde{A}$ 
            \State $A \gets $ Random Matrix(k,D)
            \State $L \gets \sum_{i=1}^D\sum_{j=1}^D[B-A^TA]_{i,j}^2$
            \State $\alpha \gets $ a sufficiently small number
            \While{$L \neq 0$}
            \State $G= 4A(A^TA-B)$
            \State $A = A - \alpha G$
            \State $L \gets \sum_{i=1}^D\sum_{j=1}^D[B-A^TA]_{i,j}^2$
            \EndWhile
        \end{algorithmic}
    \end{algorithm}  
    \begin{flushleft}
        The Python code for the algorithm above is filed as "Gradient\_Descent.py"
    \end{flushleft}
    \begin{flushleft}
        \justifying
        About $\alpha$, different scales of $\alpha$ $(0.1,0.01,0.001)$ have been tried to observe if they are sufficiently small enough that the Loss can converge to $0$. When the $\alpha = \{0.1, 0.01\}$, the Loss cannot converge and keep growing to infinity. When $\alpha = 0.001$, the Loss can converge to 0. However, the Loss will not converge to exactly 0 in real-time programming, so the program will be terminated when Loss is very close to 0.
    \end{flushleft}
    \begin{flushleft}
        \justifying
        And the history of Loss during the iteration is shown below:
    \end{flushleft}
    \begin{center}
        \includegraphics[width=10cm, height=8cm]{Loss_History.png}
    \end{center}

    \item \textbf{
     For a given $B (D = 10, k = 10)$, do you recover the same $A$ every time, for different initial starting points? Why or why not.
    }\\
    \begin{flushleft}
        \justifying
        The answer is no. The easy explanation for this is we can easily generate different $A$ that yield the same $B$:
    
    
    \begin{align*}
        \text{Let:} \quad A_{i,j} = 1, \quad i,j = 1,2,\dots,D\\
        \quad \Bar{A}_{i,j} = -1, \quad i,j = 1,2,\dots,D\\
    \end{align*}
    
    \begin{align*}
        \Rightarrow \quad B=A^TA=\Bar{A}^T\Bar{A},\quad B_{i,j} = 1, \quad i,j = 1,2,\dots,D
    \end{align*}
    
    \end{flushleft}

    \begin{flushleft}
            \justifying And interestingly, if we rotate $A$ with a rotation matrix $M$ to generate $\Bar{A}$, we will find that $\Bar{A}^T\Bar{A} = A^TA = B$. This is Because:
            \begin{align*}
                \Bar{A}^T\Bar{A} = (MA)^T(MA) = A^TM^TMA = A^T(M^TM)A =A^TIA = A^TA 
            \end{align*}
    \end{flushleft}

    \item 
    Generating $B$ randomly as above, with $D = 5, k = 10$, suppose that the ‘true’ value of $k$ is forgotten. 
    \textbf{
     Try to find $A$ for different values of $k$. What do you notice about the loss for different $k$? Can you recover the ‘true’ dimension?
    }
    \begin{flushleft}
        \justifying
        By altering the code of "Gradient\_Descent.py", we can try to recover $B$ with different $A$ with different $k$. (The code is filed as 2\_d.py) And the graph of their lost during the gradient descent is shown below:
    \end{flushleft}
    \begin{center}
        \includegraphics[width=10cm, height=8cm]{Loss for different k.png}
    \end{center}
    \begin{flushleft}
        \justifying
        We can see that with different $k$, we can still recover $B$. So we cannot recover the 'true' dimension. And by observing the Loss of different $k$, we can see that Losses for bigger $k$ converge more quickly to 0. 
    \end{flushleft}

    \item 
    Think about the relationship between the columns of $A$ and the matrix $A^T A$, and
    \textbf{
     use this to explain the results of the previous two bullet points.
    }
    \begin{flushleft}
        \justifying
        we have:
        \begin{align*}
            [A^TA]_{i,j} = \underline a_i \cdot \underline a_j
        \end{align*}
        So the entries of $A^TA$ represent the dot products between columns of A, which means that the matrix $A^TA$ represent the relationship between columns of A.
        \\
        In this case, we can try to use this information to explain bullet points (c) and (d):
        \begin{flushleft}
            \justifying
            For (c): We can recover $B$ with different $A$ because any $A$ has the columns relationships represented by $B$ can generate $B$ by $A^TA$. 
        \end{flushleft}
        \begin{flushleft}
            \justifying
            For (d): We cannot find the true $k$ because $B$ only represent the columns relationships between each $A$'s columns. It has nothing to do with the dimension of the $A$'s columns.
        \end{flushleft}
        
    \end{flushleft}

    \item 
    \textbf{
    What happens if you try to take $B$ as the identity matrix? What does the solution $A$ represent (for any $k$ that works)?
    }
    

    
    \begin{align*}
        B &= I \\
        \Rightarrow \quad \underline a_i \cdot \underline a_j &= B_{i,j} = 
        \begin{cases}
             1 \quad i=j \\
             0 \quad i\not=j  
        \end{cases}
    \end{align*}
    \begin{align*}
        \Rightarrow \quad \text{the columns in $A$ are \textbf{Orthonormal} to each other}
    \end{align*}
    
    \item 
    \textbf{
     What happens if you try to take $B$ as the diagonal matrix of all 1s, except for bottom right corner which is $-1$? What does the solution $A$ represent (for any $k$ that works)?
    }
    \begin{align*}
        \underline a_i \cdot \underline a_j &= B_{i,j} = 
        \begin{cases}
             1 \quad i=j\not=D \\
             -1 \quad i=j=D \\
             0 \quad i\not=j  
        \end{cases}
    \end{align*}
    
    \begin{flushleft}
        \justifying
         I've tried to recover the $B$ with different $k$ but failed. The Loss stop descending after it reach $1$. And the B only can be recovered as the diagonal matrix of all 1s, except for bottom right corner which is $0$. 
    \end{flushleft}
    
    \begin{flushleft}
        \justifying
         So I don't think there is an $A$ can recover $B$. Because what $B$ tells us is that the dot product of last column of $A$ and itself is $-1$, which is impossible because this represents the norm of $\underline a_D$ and the norm of a vector cannot be negative.
    \end{flushleft}

    \begin{flushleft}
        \justifying
         And actually, I don't think there is an $A$ to recover $B$ if there is a negative entire on $B$'s diagonal.
    \end{flushleft}
    \newpage
    
\end{enumerate}
\bigskip

\item Show that if $M$ preserves norms under multiplication ($||M \underline v||_2 = ||\underline v||_2$ for all $\underline v$), then the columns of $M$ must be orthonormal with respect to each other.\\
\begin{flushleft}
    \justifying
    Firstly, we have:
    \begin{align*}
        ||M\underline v||_2^2 &= (M\underline v)^T(M\underline v)\\
        &= \underline v^T M^T M \underline v
    \end{align*}
    \begin{align*}
        \text{Let:} \quad M' = M^TM, M &= 
                    \begin{bmatrix}
                         | & | & | & | \\
                         \underline m_1 & \underline m_2 & \dots & \underline m_d\\
                         | & | & | & |
                    \end{bmatrix}\\
        \Rightarrow \quad M'_{i,j} &= \underline m_i \cdot \underline m_j 
    \end{align*}
    Because $||M \underline v||_2 = ||\underline v||_2$ for all $\underline v$, we can:
    \begin{align*}
        \text{Let:} \quad \underline v_i = 
        \begin{cases}
            1; \quad i=k \\
            0; \quad \text{elsewhere}
        \end{cases},  \text{$k$ is a constant and }k\in \{1,2,\dots,D\}
    \end{align*}
    \begin{align*}
        &\Rightarrow \quad \underline v^T M^T M \underline v = 
        \underline v^T M' \underline v 
        = \underline m_k \cdot \underline m_k
        = ||\underline m_k||_2
        = ||\underline v||_2 = 1\\
        &\Rightarrow \quad \text{The columns of $M$ are normal vectors}
    \end{align*}
    Then we can:
    \begin{align*}
        \text{Let:} \quad \underline v_i = 
        \begin{cases}
            1; \quad i=k \\
            1; \quad i=k'\\
            0; \quad \text{elsewhere}
        \end{cases}; \text{ $k,k'$ is constants that }k,k'\in \{1,2,\dots,D\}
        \text{, and $k\not=k'$}
    \end{align*}
    \begin{align*}
        \Rightarrow \quad \underline v^T M^T M \underline v &= 
        \underline v^T M' \underline v \\
        &= \underline m_k \cdot \underline m_k + \underline m_{k'} \cdot \underline m_{k'} 
        + \underline m_{k'} \cdot \underline m_k + \underline m_k \cdot \underline m_{k'}
        \\
        &= 2 + 2(\underline m_k \cdot \underline m_{k'})
    \end{align*}
    And we Have:
     \begin{align*}
          &v^T M^T M \underline v = ||\underline v|| = 2
          \\
          &\Rightarrow \quad \underline m_k \cdot \underline m_{k'} = 0
          \\
          &\Rightarrow \text{The columns of $M$ are \textbf{Orthogonal} to each other}
    \end{align*}
    So, if $M$ preserves norms under multiplication ($||M \underline v||_2 = ||\underline v||_2$ for all $\underline v$), then the columns of $M$ must be \textbf{orthonormal} with respect to each other.
\end{flushleft}
\newpage

\item 
\begin{enumerate}
    \item 
    \textbf{Show that if $\underline w^T$ denotes the solution at time $T$ of this problem, then:}
    \begin{flushleft}
        \justifying
        \begin{align*}
            \underline\omega = R_T^{-1}U_T, \text{ where } R_T = \sum_{t=1}^T \underline x_t \underline x_t^T, U_t = \sum_{t=1}^Ty_t \underline x_t
        \end{align*}
    \end{flushleft}
    if $\underline \omega^T$ denotes the solution, we can know that:
    \begin{align*}
        \frac{\partial \sum_{t=1}^T (\underline x_t^T \omega - y_t)^2}{\partial \omega} = 0
    \end{align*}
    \begin{align*}
        \frac{\partial \sum_{t=1}^T (\underline x_t^T \omega - y_t)^2}{\partial \omega} &= \frac{\partial \sum_{t=1}^T ((\underline x_t^T \omega)^2 - 2y_t \underline x_t^T\omega) }{\partial \omega} \\
        &= \frac{\partial \sum_{t=1}^T (\underline x_t^T \omega)^2 }{\partial \omega}
        - 2\frac{\partial \sum_{t=1}^T  y_t \underline x_t^T\omega }{\partial \omega}\\
        &= 2\sum_{t=1}^T \underline x_t \underline x_t^T \underline \omega_T - 2\sum_{t=1}^Ty_t \underline x_t \\
        &= 2R_T\underline \omega_T - 2U_T\\
        &=0
    \end{align*}
    \begin{align*}
        &\Rightarrow \quad 2R_T\underline \omega_T = 2U_T\\
        &\Rightarrow \quad \underline\omega = R_T^{-1}U_T
    \end{align*}

    \item 
    \textbf{Assume that $R_T^{-1}$ has been computed. Express $R_{T+1}^{-1}$ in terms of $R_T^{-1}$ and $\underline x_{t+1}$.}
    \begin{align*}
        R_T &= \sum_{t=1}^T \underline x_t \underline x_t^T \\
        \Rightarrow \quad R_{T+1} &= R_T + \underline x_{t+1} \underline x_{t+1}^T
    \end{align*}
    By using the Sherman–Morrison formula:
    \begin{align*}
         R_{T+1}^{-1} &= (R_T + \underline x_{t+1} \underline x_{t+1}^T)^{-1} \\
         &= R_{T}^{-1} - \frac{R_{T}^{-1} \underline x_{t+1} \underline x_{t+1}^T R_{T}^{-1} }
         {1 +  \underline x_{t+1}^T R_{T}^{-1} \underline x_{t+1}}
    \end{align*}

    \item 
    Rather than re-computing a new model $\underline \omega_t$ at each timestep $t$, it would be better if we could \textit{update} our previous model to reflect the new data \textbf{Show that:}
    \begin{align*}
        \underline \omega_{T+1} = \underline \omega_{T} + (y_{t+1} - \underline x_{t+1}^T \underline \omega_T)K_{T+1}
    \end{align*}
    Let:
    \begin{align*}
        K'_{T+1} = \frac{R_{T}^{-1} \underline x_{t+1} \underline x_{t+1}^T R_{T}^{-1} }
         {1 +  \underline x_{t+1}^T R_{T}^{-1} \underline x_{t+1}}
    \end{align*}
    \begin{align*}
        \Rightarrow \underline \omega_{T+1} &= (R_T^{-1} - K'_{T+1})(U_T + y_{t+1}\underline x_{t+1})\\
        &= R_T^{-1}U_T + R_T^{-1}y_{t+1}\underline x_{t+1} - K'_{T+1}(U_T + y_{t+1}\underline x_{t+1})\\
        &= \underline \omega_t + y_{t+1}R_T^{-1}\underline x_{t+1} - 
        \frac{R_{T}^{-1} \underline x_{t+1} }
         {1 +  \underline x_{t+1}^T R_{T}^{-1} \underline x_{t+1}}
         \underline x_{t+1}^T \underline \omega_T -
         \frac{R_{T}^{-1} \underline x_{t+1} \underline x_{t+1}^T R_{T}^{-1} }
         {1 +  \underline x_{t+1}^T R_{T}^{-1} \underline x_{t+1}} y_{t+1}\underline x_{t+1}\\
         &= 
        \underline \omega_t 
        + 
        \frac{y_{t+1}R_T^{-1}\underline x_{t+1} + y_{t+1}R_T^{-1}\underline x_{t+1}\underline x_{t+1}^T R_{T}^{-1}\underline x_{t+1}}
        {1 +  \underline x_{t+1}^T R_{T}^{-1}\underline x_{t+1}}
        - 
        \frac{R_{T}^{-1} \underline x_{t+1} }
         {1 +  \underline x_{t+1}^T R_{T}^{-1} \underline x_{t+1}}
         \underline x_{t+1}^T \underline \omega_T \\
         &-
         \frac{R_{T}^{-1} \underline x_{t+1} \underline x_{t+1}^T R_{T}^{-1} }
         {1 +  \underline x_{t+1}^T R_{T}^{-1} \underline x_{t+1}} y_{t+1}\underline x_{t+1}\\
         &= \underline \omega_t + \frac{y_{t+1}R_T^{-1}\underline x_{t+1} - R_{T}^{-1} \underline x_{t+1}\underline x_{t+1}^T \underline \omega_T}{1 +  \underline x_{t+1}^T R_{T}^{-1} \underline x_{t+1}}\\
         &= \underline \omega_t + (y_{t+1}-x_{t+1}^T \underline \omega_T)\frac{R_{T}^{-1} \underline x_{t+1}}{1 +  \underline x_{t+1}^T R_{T}^{-1} \underline x_{t+1}}
    \end{align*}
    \textbf{HELL YEAH I DID IT!}
    \begin{align*}
        \Rightarrow \quad K_{T+1}=\frac{R_{T}^{-1} \underline x_{t+1}}{1 +  \underline x_{t+1}^T R_{T}^{-1} \underline x_{t+1}}
    \end{align*}




    
\end{enumerate}

\newpage
\item 
Consider the matrix
\begin{align*}
    A = \begin{bmatrix}
                         1 & 0.5 \\
                         0 & 1+\epsilon
    \end{bmatrix}
\end{align*}

\begin{enumerate}
    \item 
    Find the eigenvalues and eigenvectors of $A$, for $ \epsilon \not = 0$, taking the eigenvectors to be of unit norm.
    \begin{flushleft}
        In order to find eigenvalues $\lambda$ and eigenvectors $\underline v$ of $A$, we have:
        \begin{align*}
            det(A - \lambda I) = 0 \quad \Leftrightarrow \quad (1-\lambda)(1+\epsilon-\lambda) = 0
        \end{align*}
        so the eigenvalues of $A$ are $\lambda_1 = 1$ and $\lambda_2 = 1+\epsilon$, and then we can compute the unit eigenvectors:
    \end{flushleft}

    \begin{align*}
            \begin{bmatrix}
                         0 & 0.5 \\
                         0 & \epsilon
            \end{bmatrix} \cdot \underline v_1 = 0, \quad
            \begin{bmatrix}
                         -\epsilon & 0.5 \\
                         0 & 0
            \end{bmatrix} \cdot \underline v_2 = 0,
    \end{align*}
    \begin{align*}
            \Rightarrow \quad 
            \underline v_1 = \begin{bmatrix}
                             1 \\
                             0 
                            \end{bmatrix}, \quad
            \underline v_1 = \begin{bmatrix}
                             \frac{1}{\sqrt{4\epsilon^2 + 1} } \\
                             \frac{2\epsilon}{\sqrt{4\epsilon^2 + 1}} 
                            \end{bmatrix}
    \end{align*}

    \item 
     Diagonalize $A$ in terms of these eigenvalues and eigenvectors.
    \begin{flushleft}
        Let:
        \begin{align*}
            V  = \begin{bmatrix}
                             1 & \frac{1}{\sqrt{4\epsilon^2 + 1} }\\
                             0 & \frac{2\epsilon}{\sqrt{4\epsilon^2 + 1}}
                            \end{bmatrix}, \quad 
            \triangle  = \begin{bmatrix}
                             1 & 0 \\
                             0 & 1+\epsilon
                            \end{bmatrix} 
        \end{align*}
        \begin{align*}
            \Rightarrow \quad A = V\triangle V^{-1}
        \end{align*}
    \end{flushleft}

    \item 
     Taking the limit as $\epsilon$ decreases from above to $0$, what happens to the diagonalization matrices? What can you conclude when $\epsilon = 0$?
     \begin{flushleft}
         When Taking the limit as $\epsilon$ decreases from above to $0$, we have:
         \begin{align*}
             \lim_{\epsilon \to 0}V  = 
                            \begin{bmatrix}
                             1 & 1\\
                             0 & 0
                            \end{bmatrix}, \quad\text{which is not invertible because: }  \det(\lim_{\epsilon \to 0}V)=0
         \end{align*}
         So we can conclude that when $\epsilon =0$, A is not diagonalizable.
     \end{flushleft}

    
\end{enumerate}





\end{enumerate}
\end{document}

